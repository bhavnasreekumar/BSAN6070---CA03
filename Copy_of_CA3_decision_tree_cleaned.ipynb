{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CA03 \u2013 Decision Tree **Algorithm**"
   ],
   "metadata": {
    "id": "xKm53Ejgb7XZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **1. Data Source and Contents**"
   ],
   "metadata": {
    "id": "VHPxqNl9oD_n"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVGxS0DmQkHj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "url = \"https://github.com/ArinB/MSBA-CA-03-Decision-Trees/blob/master/census_data.csv?raw=true\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "id": "e3HC9tPQSkzs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **2. Data Quality Analysis (DQA)**"
   ],
   "metadata": {
    "id": "6LlIGMhpol79"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Basic structure\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nData Types:\\n\", df.dtypes)\n"
   ],
   "metadata": {
    "id": "J2suwtcKS6jT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\\n\", df.describe())\n"
   ],
   "metadata": {
    "id": "7jYoETWZTSTR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Missing values\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())"
   ],
   "metadata": {
    "id": "rxm1rdMDTUU-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check duplicates\n",
    "print(\"\\nDuplicate Rows:\", df.duplicated().sum())"
   ],
   "metadata": {
    "id": "eVH1FVRbTVwi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#using ydataprofiling library to generate report and correlation matrix.\n",
    "!pip install ydata-profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df, title=\"Census Data Report\", explorative=True)\n",
    "profile.to_file(\"census_report.html\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"census_report.html\")\n",
    "#file will download. Correlation matrix and bar charts of frequency per instance are included."
   ],
   "metadata": {
    "id": "Ry_exbXLToH9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#descriptive statistics for each column\n",
    "df.describe(include='all')"
   ],
   "metadata": {
    "id": "7DnegQdwbWSM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q.1 Why does it makes sense to discretize columns for this problem?**\n",
    "\n",
    "It makes sense to discretize columns for this problem because several features, such as age, hours worked per week, and capital gains, are continuous numerical variables. If these values were left continuous, the decision tree could create splits based on very specific numeric thresholds, resulting in a large number of branches and a more complex tree. By discretizing these values into bins, ranges of values are grouped together, reducing unnecessary splits and simplifying the model. This also reflects real-world reasoning, where small differences (for example, between a 50-year-old and a 51-year-old) are unlikely to meaningfully affect income outcomes. Overall, discretization helps reduce model complexity and lowers the risk of overfitting.\n",
    "\n",
    "**Q.2 What might be the issues (if any) if we DID NOT discretize the column**\n",
    "\n",
    "If the columns were not discretized, the decision tree would be more prone to overfitting, as it could create splits based on every distinct numeric value. Small variations in age, capital gain, or hours worked per week, which may have minimal real-world impact, could lead to very different split decisions in the tree. This would result in a highly complex model that fits the training data too closely and may not generalize well to new data."
   ],
   "metadata": {
    "id": "xUYVFOnBSu0C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#data quality report\n",
    "dq_report = pd.DataFrame({\n",
    "    'Data Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Missing Values': df.isnull().sum(),\n",
    "    'Missing %': (df.isnull().sum() / len(df)) * 100,\n",
    "    'Unique Values': df.nunique()\n",
    "})\n"
   ],
   "metadata": {
    "id": "lpXuQGMFbjA6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#outlier check\n",
    "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "Q1 = numeric_df.quantile(0.25)\n",
    "Q3 = numeric_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = ((numeric_df < lower_bound) | (numeric_df > upper_bound))\n",
    "\n",
    "dq_report['Outlier'] = outliers.sum()\n",
    "dq_report\n"
   ],
   "metadata": {
    "id": "mwtNEJUpbve4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data** **cleaning**\n",
    "\n",
    "This data set is based on \"bins\" and not continuous variables so it is likely there will be duplicates as each variable is in a category rather than being represented by the individual number. Dropping the duplicates will skew the data as the duplicates represents categories that often appear together."
   ],
   "metadata": {
    "id": "RtCa3yG4Syy5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#data cleaning\n",
    "\n",
    "#dropping one of the education columns since highly correlated\n",
    "df.drop('education_num_bin', axis=1, inplace=True)\n"
   ],
   "metadata": {
    "id": "n6PdiKikdJ92"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#label encoding and splitting the data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_income = LabelEncoder()\n",
    "df['y'] = le_income.fit_transform(df['y'])\n"
   ],
   "metadata": {
    "id": "ABV_Qpq2mRzV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "label_encoders = {}\n",
    "\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "# remove flag from encoding if it's categorical\n",
    "categorical_cols = categorical_cols.drop('flag')\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n"
   ],
   "metadata": {
    "id": "g0fFlt17mUxf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = df[df['flag'] == 'train']\n",
    "test_df  = df[df['flag'] == 'test']\n",
    "\n",
    "x_train = train_df.drop(['y', 'flag'], axis=1)\n",
    "y_train = train_df.y\n",
    "\n",
    "x_test = test_df.drop(['y', 'flag'], axis=1)\n",
    "y_test = test_df.y\n"
   ],
   "metadata": {
    "id": "6fency2omZL1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **3. Build Decision Tree Classifier Models**"
   ],
   "metadata": {
    "id": "wV_Wgkq4pEIM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    random_state=101,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=15\n",
    ")\n",
    "\n",
    "dtree.fit(x_train, y_train)\n",
    "\n",
    "y_pred = dtree.predict(x_test)"
   ],
   "metadata": {
    "id": "YwD-refsmbTq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **4. Evaluate Decision Tree Performance**"
   ],
   "metadata": {
    "id": "JrlGz8aQpNOw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ],
   "metadata": {
    "id": "BGXo66l_n77f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#extracting results\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "print(\"True Negatives (TN):\", TN)\n",
    "print(\"False Positives (FP):\", FP)\n",
    "print(\"False Negatives (FN):\", FN)\n",
    "print(\"True Positives (TP):\", TP)\n"
   ],
   "metadata": {
    "id": "NzftesNpoA55"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "id": "EhnpMcrOoFYB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)"
   ],
   "metadata": {
    "id": "mfjwsPgvoHM2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)"
   ],
   "metadata": {
    "id": "b_WViiRloI4M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)"
   ],
   "metadata": {
    "id": "c7XRC-7loKPY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **5. Tune Decision Tree Performance**"
   ],
   "metadata": {
    "id": "CiWvxserpZSX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q.3 Decision Tree Hyper-parameter variation vs. performance**"
   ],
   "metadata": {
    "id": "Fww0MbLGpdPQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "run1_results = []\n",
    "\n",
    "for criter in [\"gini\", \"entropy\"]:\n",
    "\n",
    "    classf = DecisionTreeClassifier(\n",
    "        criterion=criter,\n",
    "        random_state=101\n",
    "    )\n",
    "\n",
    "    classf.fit(x_train, y_train)\n",
    "    pred = classf.predict(x_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "    run1_results.append({\n",
    "        \"criterion\": criter,\n",
    "        \"accuracy\": accuracy_score(y_test, pred),\n",
    "        \"precision\": precision_score(y_test, pred),\n",
    "        \"recall\": recall_score(y_test, pred),\n",
    "        \"f1\": f1_score(y_test, pred),\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "run1_df = pd.DataFrame(run1_results)\n",
    "display(run1_df)\n",
    "\n",
    "best_criterion = run1_df.sort_values(\"accuracy\", ascending=False).iloc[0][\"criterion\"]\n",
    "print(\"Best criterion:\", best_criterion)"
   ],
   "metadata": {
    "id": "F2Lzn0aqpzPg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "run2_results = []\n",
    "\n",
    "for leaf in [5, 10, 15, 20, 25, 30, 35, 40]:\n",
    "\n",
    "    classf = DecisionTreeClassifier(\n",
    "        criterion=best_criterion,\n",
    "        min_samples_leaf=leaf,\n",
    "        random_state=101\n",
    "    )\n",
    "\n",
    "    classf.fit(x_train, y_train)\n",
    "    pred = classf.predict(x_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "    run2_results.append({\n",
    "        \"min_samples_leaf\": leaf,\n",
    "        \"accuracy\": accuracy_score(y_test, pred),\n",
    "        \"precision\": precision_score(y_test, pred),\n",
    "        \"recall\": recall_score(y_test, pred),\n",
    "        \"f1\": f1_score(y_test, pred),\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "run2_df = pd.DataFrame(run2_results)\n",
    "display(run2_df)\n",
    "\n",
    "best_leaf = int(run2_df.sort_values(\"accuracy\", ascending=False).iloc[0][\"min_samples_leaf\"])\n",
    "print(\"Best min_samples_leaf:\", best_leaf)"
   ],
   "metadata": {
    "id": "LSFsd6ERqy7o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(run2_df[\"min_samples_leaf\"], run2_df[\"accuracy\"], marker=\"o\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Run 2: min_samples_leaf vs accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "NzLi8wyJqZDq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "run3_results = []\n",
    "\n",
    "for mf in [None, \"sqrt\", 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "\n",
    "    classf = DecisionTreeClassifier(\n",
    "        criterion=best_criterion,\n",
    "        min_samples_leaf=best_leaf,\n",
    "        max_features=mf,\n",
    "        random_state=101\n",
    "    )\n",
    "\n",
    "    classf.fit(x_train, y_train)\n",
    "    pred = classf.predict(x_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "    run3_results.append({\n",
    "        \"max_features\": mf,\n",
    "        \"accuracy\": accuracy_score(y_test, pred),\n",
    "        \"precision\": precision_score(y_test, pred),\n",
    "        \"recall\": recall_score(y_test, pred),\n",
    "        \"f1\": f1_score(y_test, pred),\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "run3_df = pd.DataFrame(run3_results)\n",
    "display(run3_df)\n",
    "\n",
    "best_max_features = run3_df.sort_values(\"accuracy\", ascending=False).iloc[0][\"max_features\"]\n",
    "print(\"Best max_features:\", best_max_features)\n"
   ],
   "metadata": {
    "id": "rADULYJmraN9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(run3_df[\"max_features\"].astype(str), run3_df[\"accuracy\"], marker=\"o\")\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Run 3: max_features vs accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "8esX3r3kr1J4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "run4_results = []\n",
    "\n",
    "for depth in [2, 4, 6, 8, 10, 12, 14, 16]:\n",
    "\n",
    "    classf = DecisionTreeClassifier(\n",
    "        criterion=best_criterion,\n",
    "        min_samples_leaf=best_leaf,\n",
    "        max_features=best_max_features,\n",
    "        max_depth=depth,\n",
    "        random_state=101\n",
    "    )\n",
    "\n",
    "    classf.fit(x_train, y_train)\n",
    "    pred = classf.predict(x_test)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "    run4_results.append({\n",
    "        \"max_depth\": depth,\n",
    "        \"accuracy\": accuracy_score(y_test, pred),\n",
    "        \"precision\": precision_score(y_test, pred),\n",
    "        \"recall\": recall_score(y_test, pred),\n",
    "        \"f1\": f1_score(y_test, pred),\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "run4_df = pd.DataFrame(run4_results)\n",
    "display(run4_df)\n",
    "\n",
    "best_depth = int(run4_df.sort_values(\"accuracy\", ascending=False).iloc[0][\"max_depth\"])\n",
    "print(\"Best max_depth:\", best_depth)"
   ],
   "metadata": {
    "id": "aqP4dy99r5lt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(run4_df[\"max_depth\"], run4_df[\"accuracy\"], marker=\"o\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Run 4: max_depth vs accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "z7w6hXrzr9SN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "final_model = DecisionTreeClassifier(\n",
    "    criterion=best_criterion,\n",
    "    min_samples_leaf=best_leaf,\n",
    "    max_features=best_max_features,\n",
    "    max_depth=best_depth,\n",
    "    random_state=101\n",
    ")\n",
    "\n",
    "final_model.fit(x_train, y_train)\n",
    "final_pred = final_model.predict(x_test)\n",
    "\n",
    "print(\"Final Accuracy:\", accuracy_score(y_test, final_pred))\n",
    "print(\"Best Parameters:\")\n",
    "print(\"criterion =\", best_criterion)\n",
    "print(\"min_samples_leaf =\", best_leaf)\n",
    "print(\"max_features =\", best_max_features)\n",
    "print(\"max_depth =\", best_depth)"
   ],
   "metadata": {
    "id": "1309NE63r_ww"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "final_model.fit(x_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(\"Total training time (seconds):\", total_time)\n"
   ],
   "metadata": {
    "id": "C_FI8tfNM5fI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install graphviz"
   ],
   "metadata": {
    "id": "UJMYsvcs0WHu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **6. Visualize Your Best Decision Tree using GraphViz**"
   ],
   "metadata": {
    "id": "ly_ZIJdGqCy5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "safe_feature_names = [re.sub(r'[^0-9a-zA-Z_]+', '_', str(c)) for c in x_train.columns]\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    final_model,\n",
    "    out_file=None,\n",
    "    feature_names=safe_feature_names,\n",
    "    class_names=[\"LE_50K\", \"GT_50K\"],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=False\n",
    ")\n",
    "\n",
    "graphviz.Source(dot_data)\n",
    "\n",
    "#the visual is too large to upload to github. Please run the code to see the final decision tree visualization."
   ],
   "metadata": {
    "id": "GRi_8ws_0Tdr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **7. Conclusion**"
   ],
   "metadata": {
    "id": "nAWx9hBOqIzw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q.4 How long was your total run time to train the best model?**\n",
    "\n",
    "Based on the time measurement cell in our notebook, the total run time to train the best-performing decision tree was approximately 0.021 seconds. Given the dataset size and the selected hyperparameters, this indicates that training was computationally efficient. Since decision trees do not require iterative optimization like some other models, the training process was relatively fast even with tuning applied.\n",
    "\n",
    "**Q.5 Did you find the BEST TREE?**\n",
    "\n",
    "Not necessarily. We selected the best-performing tree from the specific hyperparameter combinations we tested (including max_depth = 16, min_samples_leaf = 40, max_features = 0.6, and criterion = gini). However, this does not guarantee that it is the absolute best possible tree overall.\n",
    "\n",
    "There are additional hyperparameters (such as min_samples_split, max_leaf_nodes, or different depth limits) that were not explored in this analysis. It is possible that a different combination outside our tested grid could achieve better performance. The final model achieved approximately 82.35% test accuracy, which shows strong predictive ability but confirms that the model is not perfect and could potentially be further optimized.\n",
    "\n",
    "**Q.6 Write your observations from the visualization of the best tree**\n",
    "\n",
    "From the visualization of the best-performing decision tree, the root node begins with the MSR (Marriage Status & Relationship) bin, indicating that it was the most informative feature for the first split based on Gini impurity. If that condition is false, the tree evaluates capital gains; if true, it moves to occupation_bin. This suggests that relationship status, capital gains, and occupation are strong predictors of income category in this dataset.\n",
    "\n",
    "The tree was limited to a maximum depth of 16, which was one of the selected hyperparameter constraints. Although the tree is relatively deep, several terminal nodes still show non-zero Gini impurity values, meaning the leaves are not perfectly pure. This indicates that the model does not completely separate the income classes.\n",
    "\n",
    "Additionally, occupation_bin appears more frequently in upper and mid-level splits compared to race_sex_bin, suggesting occupation has greater predictive influence. Toward the bottom of the tree, age_bin appears more often, indicating that age refines predictions after broader splits have already been made.\n",
    "\n",
    "**Q.7 Will this Tree \u201coverfit\u201d? (Hint: Is this tree \u201cfully grown\u201d)**\n",
    "\n",
    "This tree is not fully grown. A fully grown decision tree would have no constraints on parameters such as max_depth, min_samples_leaf, or min_samples_split, allowing it to continue splitting until all leaves are pure.\n",
    "\n",
    "In our model, we imposed constraints including max_depth = 16 and min_samples_leaf = 40, which prevent excessive splitting. Because of these limitations, the tree cannot perfectly memorize the training data. The presence of non-zero Gini values in the leaf nodes further confirms that the model is not fully grown. While some risk of overfitting may still exist due to the tree\u2019s depth, the imposed constraints help reduce that risk."
   ],
   "metadata": {
    "id": "dE9IMMMY2Cbt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **8. Prediction using your \u201ctrained\u201d Decision Tree Mode**"
   ],
   "metadata": {
    "id": "ZutSUgtsrdYd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "model = final_model\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Normalize strings so tiny dash differences don\u2019t break matching.\"\"\"\n",
    "    return (str(s)\n",
    "            .replace(\"\u2013\", \"-\")\n",
    "            .replace(\"\u2014\", \"-\")\n",
    "            .replace(\"  \", \" \")\n",
    "            .strip()\n",
    "            .lower())\n",
    "\n",
    "def pick_label_contains(col, target_text):\n",
    "    \"\"\"Pick the EXACT label from label_encoders[col].classes_ that contains target_text.\"\"\"\n",
    "    target = normalize(target_text)\n",
    "    classes = list(label_encoders[col].classes_)\n",
    "    for c in classes:\n",
    "        if target in normalize(c):\n",
    "            return c\n",
    "    raise ValueError(f\"Could not find a class in {col} containing '{target_text}'. Classes were: {classes}\")\n",
    "\n",
    "def pick_range_label(value, classes):\n",
    "    \"\"\"\n",
    "    Map a numeric value to a class label like:\n",
    "    'a. 0-30' or 'd. 41-50 & 61-70'\n",
    "    \"\"\"\n",
    "    v = float(value)\n",
    "    for lab in classes:\n",
    "        txt = str(lab)\n",
    "\n",
    "        # find all ranges like 41-50 or 61-70 inside the label\n",
    "        ranges = re.findall(r'(\\d+)\\s*-\\s*(\\d+)', txt)\n",
    "        for lo, hi in ranges:\n",
    "            lo, hi = float(lo), float(hi)\n",
    "            if lo <= v <= hi:\n",
    "                return lab\n",
    "\n",
    "    raise ValueError(f\"Could not map value={value} to any bin label in: {list(classes)}\")\n",
    "\n",
    "# 1) Build the row using the assignment\u2019s info\n",
    "row_labels = {}\n",
    "\n",
    "# hours worked per week = 48\n",
    "row_labels[\"hours_per_week_bin\"] = pick_range_label(\n",
    "    48, label_encoders[\"hours_per_week_bin\"].classes_\n",
    ")\n",
    "\n",
    "# occupation = Mid - Low\n",
    "row_labels[\"occupation_bin\"] = pick_label_contains(\"occupation_bin\", \"Mid - Low\")\n",
    "\n",
    "# marriage status & relationships = High\n",
    "row_labels[\"msr_bin\"] = pick_label_contains(\"msr_bin\", \"High\")\n",
    "\n",
    "# capital gain = Yes -> capital_gl_bin classes look like: 'a. = 0', 'b. < 0', 'c. > 0'\n",
    "row_labels[\"capital_gl_bin\"] = pick_label_contains(\"capital_gl_bin\", \"> 0\")\n",
    "\n",
    "# race-sex group = Mid\n",
    "row_labels[\"race_sex_bin\"] = pick_label_contains(\"race_sex_bin\", \"Mid\")\n",
    "\n",
    "# education category = High\n",
    "row_labels[\"education_bin\"] = pick_label_contains(\"education_bin\", \"High\")\n",
    "\n",
    "# work class = Income\n",
    "row_labels[\"workclass_bin\"] = pick_label_contains(\"workclass_bin\", \"income\")\n",
    "\n",
    "# age = 58\n",
    "row_labels[\"age_bin\"] = pick_range_label(\n",
    "    58, label_encoders[\"age_bin\"].classes_\n",
    ")\n",
    "\n",
    "# 2) Convert to DataFrame in the SAME column order as training\n",
    "new_df = pd.DataFrame([row_labels]).reindex(columns=x_train.columns)\n",
    "\n",
    "# 3) Apply the SAME encoders used in training\n",
    "for col in new_df.columns:\n",
    "    if col in label_encoders:\n",
    "        new_df[col] = label_encoders[col].transform(new_df[col].astype(str))\n",
    "\n",
    "# 4) Predict + probability\n",
    "pred_num = model.predict(new_df)[0]\n",
    "proba = model.predict_proba(new_df)[0]          # [P(class0), P(class1)]\n",
    "\n",
    "pred_label = \"<=50K\" if pred_num == 0 else \">50K\"\n",
    "prob_correct = float(proba.max())              # probability of the predicted class\n",
    "\n",
    "print(\"Q8 Prediction:\", pred_label)\n",
    "print(\"Probability prediction is correct:\", prob_correct)\n",
    "print(\"Probabilities [<=50K, >50K]:\", proba)\n",
    "print(\"Row used (encoded):\")\n",
    "display(new_df)\n"
   ],
   "metadata": {
    "id": "fo3C-rPd93O8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "chatgpt link. Please scroll up to see project related chats.\n",
    "https://chatgpt.com/share/e/6994dd80-f6a4-800d-89c1-2dc0231349b3\n"
   ],
   "metadata": {
    "id": "zk8gwiIfgG6V"
   }
  }
 ]
}